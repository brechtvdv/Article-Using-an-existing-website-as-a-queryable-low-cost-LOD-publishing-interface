<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta charset="utf-8" />
  <title>Using an existing website as a queryable low-cost LOD publishing interface</title>
  <link rel="stylesheet" media="screen" href="styles/screen.css" />
  <link rel="stylesheet" media="print"  href="styles/print.css" />
  
</head>
<body prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs: http://www.w3.org/2000/01/rdf-schema# owl: http://www.w3.org/2002/07/owl# xsd: http://www.w3.org/2001/XMLSchema# dcterms: http://purl.org/dc/terms/ dctypes: http://purl.org/dc/dcmitype/ foaf: http://xmlns.com/foaf/0.1/ v: http://www.w3.org/2006/vcard/ns# pimspace: http://www.w3.org/ns/pim/space# cc: https://creativecommons.org/ns# skos: http://www.w3.org/2004/02/skos/core# prov: http://www.w3.org/ns/prov# qb: http://purl.org/linked-data/cube# schema: http://schema.org/ void: http://rdfs.org/ns/void# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# cal: http://www.w3.org/2002/12/cal/ical# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# org: http://www.w3.org/ns/org# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ sioc: http://rdfs.org/sioc/ns# doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# ldp: http://www.w3.org/ns/ldp# solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio# as: https://www.w3.org/ns/activitystreams# oa: http://www.w3.org/ns/oa# ldp: http://www.w3.org/ns/ldp#" typeof="schema:CreativeWork sioc:Post prov:Entity">
  <header>
  <h1 id="using-an-existing-website-as-a-queryable-low-cost-lod-publishing-interface">Using an existing website as a queryable low-cost LOD publishing interface</h1>

  <ul id="authors">
    <li><a href="https://brechtvdv.github.io/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://w3id.org/people/brechtvdv/#me">Brecht Van de Vyvere</a></li>
    <li><a href="http://www.rubensworks.net/" typeof="http://xmlns.com/foaf/0.1/Person" resource="http://www.rubensworks.net/#me">Ruben Taelman</a></li>
    <li><a href="https://pietercolpaert.be" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://pietercolpaert.be/#me">Pieter Colpaert</a></li>
    <li><a href="https://ruben.verborgh.org/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://ruben.verborgh.org/profile/#me">Ruben Verborgh</a></li>
  </ul>

  <ul id="affiliations">
    <li id="idlab">IDLab,
          Department of Electronics and Information Systems,
          Ghent University – imec</li>
  </ul>

  <section class="context">
    <h2 id="in-reply-to">In reply to</h2>
    <ul>
      <li><a href="https://2019.eswc-conferences.org/call-for-posters-and-demos/" rel="as:inReplyTo">ESWC 2019 Call for Posters and Demos</a></li>
    </ul>
  </section>

</header>

<!-- Hack to make our custom fonts load in print-mode -->
<!-- https://stackoverflow.com/questions/39364259/chrome-print-preview-doesnt-load-media-only-print-font-face -->
<p><span class="printfont1"> </span>
<span class="printfont2"> </span>
<span class="printfont3"> </span>
<span class="printfont4"> </span></p>

<div class="double-column">

<section id="abstract">
    <h2>Abstract</h2>
    <!-- Context      -->
    <p>Maintaining an Open Dataset comes at an extra recurring cost when it is published in a dedicated Web interface.
<!-- Need         -->
As there is not often a direct financial return from publishing a dataset publicly, these extra costs need to be minimized.
<!-- Task         -->
Therefore we want to explore reusing existing infrastructure by enriching existing websites with Linked Data.
<!-- Object       -->
In this demonstrator, we advised the data owner to annotate a digital heritage website with JSON-LD snippets, resulting in a dataset of more than three million triples that is now available and officially maintained. <!--TODO: how much data was in te end published? Can we have some stats about the total data dump? -->
<!--Only an initial investment is required to have Linked Data snippets added to its corresponding webpages.-->
The website itself is paged, and thus hydra partial collection view controls were added in the snippets.
We then extended the modular query engine <a href="http://comunica.linkeddatafragments.org">Comunica</a> to support following page controls and extracting data from HTML documents while querying.
<!-- Findings     -->
This way, a SPARQL or GraphQL query over multiple heterogeneous data sources can power automated data reuse.
While the query performance on such an interface is visibly poor, it becomes easy to create composite data dumps.
<!-- Conclusion and Perspectives -->
As a result of implementing these building blocks in Comunica, any paged collection and enriched HTML page now becomes queryable by the query engine.
This enables heterogenous data interfaces to share functionality and become technically interoperable.</p>

  </section>


<main>
  <!-- Add sections by specifying their file name, excluding the '.md' suffix. -->
  <section>
      <div class="printonly">This is a print-version of a paper first written for the Web. The Web-version is available at <a href="https://brechtvdv.github.io/Article-Using-an-existing-website-as-a-queryable-low-cost-LOD-publishing-interface/">https:/​/​brechtvdv.github.io/Article-Using-an-existing-website-as-a-queryable-low-cost-LOD-publishing-interface/</a>.</div>

      <h2 id="introduction">Introduction</h2>

      <p>The <a href="https://viaa.be">Flemish Institute for Audiovisual Archiving</a> (VIAA) is a non-profit organization that preserves petabytes of valuable image, audio or video files. These files and accompanying metadata are covered by distinct <a href="https://viaa.be/nl/portaal/support-category/item/viaa-licenties-in-het-archiefsysteem">licenses</a>, but some can be made accessible under an Open Data <a href="https://creativecommons.org/publicdomain/zero/1.0/">license</a>. One initiative is opening up historical newspapers of the first World War with the open platform <a href="https://hetarchief.be">hetarchief.be</a>. In 2017, the raw data of these newspapers have been published as a <a property="schema:citation http://purl.org/spar/cito/cites" href="https://5stardata.info/en/">Linked Open Data</a> <span class="references">[<a href="#ref-1">1</a>]</span> (LOD) dataset using the low-cost <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments</a></span> <span class="references">[<a href="#ref-2">2</a>]</span> (TPF) interface. Although this interface is still accessable (<a href="http://linkeddatafragments-qas.viaa.be/">http:/​/​linkeddatafragments-qas.viaa.be/</a>), no updates from the website have been exported to the TPF interface due to absence of automatisation.</p>

      <figure id="wiperstimes">
<center>
<img src="img/wiperstimes.PNG" />
</center>
<figcaption>
          <p><span class="label">Fig. 1:</span> The famous newspaper ‘Wipers Times’ published on 26th February 1916 (source: hetarchief.be).</p>
        </figcaption>
</figure>

      <p>Maintaining an up-to-date LOD interface brings besides technical resources also an organizational challenge. Content editors often work in a seperate environment such as a <a href="https://en.wikipedia.org/wiki/Content_management_system">Content Management System</a> (CMS) to update a website. The raw data gets exported from that system and published in a dedicated environment leaving the source of truth to the CMS. The question rises whether the data can be published closer to the authorative source in a more sustainable way.</p>

      <p>Website maintainers are currently using <a href="https://json-ld.org/spec/latest/json-ld/">JSON-LD</a> structured data snippets to attain better search result ranking and visualisation with search engines. These snippets are script tags inside a HTML webpage containing Linked Data in JSON format (JSON-LD) compliant with the Schema.org <span class="references">[<a href="#ref-3">3</a>]</span> datamodel. Not only <a href="https://support.google.com/webmasters/answer/3069489?hl=en">search engine optimization</a> (SEO), but also standard LOD publishing is possible. The data should be representative with the main content of the subject page, such as newspaper metadata, to be aligned with the structured data <a href="https://developers.google.com/search/docs/guides/sd-policies">guidelines</a> of search engines. In order that Linked Data user agents such as <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-030-00668-6_15"><a href="https://comunica.github.io/Article-ISWC2018-Resource/">Comunica</a></span> <span class="references">[<a href="#ref-4">4</a>]</span> can query a website as a dataset, the webpages should be linked together through hypermedia controls <span class="references">[<a href="#ref-5">5</a>]</span>.</p>

      <p>First we give a short background of the Comunica tool and the Hydra partial collection views.
We then describe how hetarchief.be is enriched with JSON-LD snippets. Next, we explain how we allow Comunica to query over this and other sources by adding two building blocks.
After this, we demonstrate how a custom data dump can be created by an end-user that wants to further analyze this data, for instance in spreadsheet software.
The online version of this paper embeds this demo and can be tested live.
Finally, we conclude the demonstrator with a discussion and perspective on future work.</p>

    </section>

  <section id="sota">
      <h2>Background</h2>

      <h3 id="comunica">Comunica</h3>

      <p><span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-030-00668-6_15"><a href="https://comunica.github.io/Article-ISWC2018-Resource/">Comunica</a></span> <span class="references">[<a href="#ref-4">4</a>]</span> is a Linked Data user agent that can run federated queries over several heterogeneous Web APIs. This engine has been developed to make it easy to plug in specific types of functionality as separate modules.</p>

      <p>A <em>bus</em> module is used as communication channel to solve one problem, e.g. to extract hypermedia controls from a Web API. Multiple <em>actor</em> modules can subscribe to a bus and extract one or more hypermedia controls according to their implementation. A <em>mediator</em> module wraps around the bus to select the most appropriate results. As such, by supporting multiple hypermedia controls more intelligent user agents can be created.</p>

      <!-- Every piece of functionality in Comunica can be implemented as seperate building blocks based on the _actor_ programming model, where each actor can respond to a specific action. Actors that share same functionality, but with different implementations, can be grouped with a communication channel called a _bus_. Interaction between actors is possible through a _mediator_ that wraps around a bus to get an action's result from a single actor. This result depends on the configuration of the mediator, e.g. a race mediator will return the response of the actor that is able to reply the earliest. -->

      <!-- <figure id="actor">
<center>
<img src="img/actor-mediator-bus.svg">
</center>
<figcaption markdown="block">
Actor 0 initiates an action to a mediator. The mediator communicates through a bus with all actors 1, 2 and 3 that are able to solve the action and gives back the most favorable result according to its configuration.
</figcaption>
</figure> -->

      <h3 id="hydra-partial-collection-views">Hydra partial collection views</h3>

      <p>Open Data is filled with collections of items (hotel amenities, road works etc.). Using the <a href="https://www.hydra-cg.com/spec/latest/core/#collections">Hydra</a> vocabulary, not only the relations between items and collections can be expressed, but also how different parts of the collection can be retrieved, so called partial collection view controls (e.g. next, previous links). This way, a collection can be split in multiple documents while a Web client can still query over the collection by retrieving its view controls.</p>

      <!-- These related resources can be grouped as members of a collection using the [Hydra](https://www.hydra-cg.com/spec/latest/core/#collections) vocabulary. When the size of members is too big, data owners can fragment this into collection views. Each view represents a part of the collection to keep the Web API responses lightweight. In the case of hetarchief.be represents each newspaper HTTP document one view of the collection of newspapers. These views are linked together with partial collection view controls: previous, next, first and last. This allows a client to fetch all members of the collection. -->
    </section>

  <section id="implementation">
      <h2>Implementation</h2>

      <h3 id="hetarchiefbe">hetarchief.be</h3>

      <p>Every newspaper webpage is annotated with JSON-LD snippets containing domain-specific metadata and hypermedia controls. The former metadata is described using acknowledged vocabularies such as <a href="http://dublincore.org/documents/dcmi-terms/">Dublin Core Terms</a> (DCTerms), <a href="http://xmlns.com/foaf/spec/">Friend of a Friend</a> (FOAF), <a href="https://schema.org/">Schema.org</a> etc. The latter is described using the <a href="https://www.hydra-cg.com/spec/latest/core">Hydra</a> vocabulary for hypermedia-driven Web APIs. Although hetarchief.be contains several human-readable hypermedia controls (free text search bar, search facets, pagination for every <a href="https://hetarchief.be/nl/media/brief-van-den-soldaat-aan-zijne-verdrukte-medeburgers/I2STYUAOpmFKmbFRXNmV0PTp">newspaper</a> ) only Hydras partial collection view controls are implemented: hydra:next describes the next newspaper, vice versa hydra:previous. Also an estimate of the amount of triples on a page is added using hydra:totalItems and void:triples. This helps user agents to build more efficient query plans.</p>

      <figure id="partial-collection-controls" class="figure">
 <pre><code>{
</code><code>  &quot;@context&quot;: &quot;https://www.w3.org/ns/hydra/context.jsonld&quot;,
</code><code>  &quot;@id&quot;: &quot;https://hetarchief.be/media/de-school-op-het-front-studiebladen-van-sursum-corda/CMEPpOVIRqYiVZSYd3Q3k8tL&quot;,
</code><code>  &quot;previous&quot;: &quot;https://hetarchief.be/media/vrij-belgi%C3%AB/B1IVhaOMLFgCUGNJkVGuZH3S&quot;,
</code><code>  &quot;next&quot;: &quot;https://hetarchief.be/media/vrij-belgi%C3%AB/J1cnCMfndMbBNrde9VxIyVpB&quot;,
</code><code>  &quot;totalItems&quot;: 50,
</code><code>  &quot;http://rdfs.org/ns/void#triples&quot;: 50
</code><code>}</code></pre>
<figcaption>
          <p><span class="label">Fig. 2:</span> Every newspaper describes its next and previous newspaper using Hydra partial collection view controls. This wires Linked Data Fragments together into a dataset.</p>
        </figcaption>
</figure>

      <!-- In order to lower the barrier for automated Open Data reuse, information responses add the [Cross-Origin Resource Sharing](https://www.w3.org/TR/cors/) (CORS) header: _Access-Control-Allow-Origin: *_ . 
Not all metadata of a newspaper falls under an Open License. In the process of digitizing these newspapers, [Optical Character Recognition](https://nl.wikipedia.org/wiki/Optical_character_recognition) (OCR) is applied. According to the [European copyright legislation](https://eur-lex.europa.eu/eli/dir/2001/29/oj) content is still reserved by default 70 years after the death of the last author. This implies that these scanned texts cannot be published provisionally as Open Data. This is solved by publishing the OCR text in a seperate Linked Data document covered by a different [terms of use](https://hetarchief.be/nl/gebruiksvoorwaarden). This also keeps the fragment size lean as such a [document](https://hetarchief.be/nl/media/brief-van-den-soldaat-aan-zijne-verdrukte-medeburgers/I2STYUAOpmFKmbFRXNmV0PTp/ocr) measures easily 50 kilobytes for four newspaper pages. -->

      <h3 id="building-blocks-comunica">Building blocks Comunica</h3>

      <p>To make Comunica work with hetarchief.be, two additional actors were needed.
First, we needed a generic actor to support pagination over any kind of hypermedia interface.
Secondly, an actor was needed to parse JSON-LD data snippets from HTML documents.
We will explain these two actors in more detail hereafter.</p>

      <p><code>BusRdfResolveHypermedia</code> is a bus in Comunica that resolves hypermedia controls from sources.
Currently, this bus only contains an actor that resolves controls for TPF interfaces.
We added a new actor (<code>ActorRdfResolveHypermediaNextPage</code>) to this bus that returns a search form containing a next page link, vice versa for previous page links.</p>

      <p>The parsing of most common Linked Data formats (<a href="https://www.w3.org/TR/turtle/">Turtle</a>, <a href="https://www.w3.org/TR/trig/">TriG</a>, <a href="https://www.w3.org/TR/rdf-syntax-grammar/">RDF/XML</a>, <a href="https://www.w3.org/2018/jsonld-cg-reports/json-ld/">JSON-LD</a>…) are already supported by Comunica.
However, no parser for extracting data snippets from HTML documents existed yet.
That is why we added an actor (<code>ActorRdfParseHtmlScript</code>) for parsing such HTML documents.
This intermediate parser searches for data snippets and forwards these to their respective RDF parser.
In case of a JSON-LD snippet, the body of a script tag  <code>&lt;script type="application/ld+json"&gt;</code> will be parsed by the JSON-LD parse actor.</p>

      <p>By adding these two actors to Comunica, we can now query over a paged collection that is declaratively described with data snippets. As federated querying comes out-of-the-box with Comunica, this cultural heritage collection can now be queried together with other knowledge bases (cfr. Wikidata). For example, retrieving basic information such as title, publication date etc. from 17 newspaper pages requires 1,5 minutes until all results are retrieved. This is caused by deficiency of indexes where all pages need examination before having a complete answer.</p>

      <!-- <figure id="federated-querying-comunica" class="listing">
<pre><code>$ node packages/actor-init-sparql/bin/query.js
</code><code>hypermedia@https://hetarchief.be/nl/media/brief-van-den-soldaat-aan-zijne-verdrukte-medeburgers/r1ROcQtfXfhOZbUbUpivGRpY
</code><code>hypermedia@https://query.wikidata.org/bigdata/ldf
</code><code>&quot;SELECT * WHERE { 
</code><code>	GRAPH ?document {
</code><code>	 ?newspaper a &lt;http://schema.org/Newspaper&gt; . 
</code><code>	 ?newspaper &lt;http://schema.org/datePublished&gt; ?datePublished . 
</code><code>	 ?newspaper &lt;http://purl.org/dc/terms/title&gt; ?title . 
</code><code>	 ?newspaper &lt;http://schema.org/publisher&gt; ?publisher . 
</code><code> 	}
</code><code>}&quot;</code></pre>
<figcaption markdown="block">
SPARQL-query over a paged collection of hetarchief.be and the TPF interface of Wikidata using the JavaScript-based command line interface of Comunica.
</figcaption>
</figure> -->

      <p>In next section we will demonstrate how SPARQL-querying can be applied for extracting a spreadsheet.</p>
    </section>

  <section id="demonstrator">
      <h2>Demonstrator</h2>

      <p>This demonstrator shows that a non-technical user can create a data dump from the cultural heritage website hetarchief.be. More specifically, a spreadsheet can be extracted using SPARQL-querying from embedded paged collection views.
The application is written with the front-end playground Codepen <a href="https://codepen.io/brechtvdv/pen/ebOzXB">https:/​/​codepen.io/brechtvdv/pen/ebOzXB</a>. A browser compatible library of Comunica is built  using a custom configuration that can be found on Github (<a href="https://github.com/brechtvdv/hetarchief-comunica">https:/​/​github.com/brechtvdv/hetarchief-comunica</a>) under an Open License.</p>

      <figure id="codepen">
<center>
<img id="codepen-img" src="img/codepen.PNG" />
</center>
<figcaption>
          <p><span class="label">Listing 1:</span> A spreadsheet is generated by entering a URL of a newspaper from hetarchief.be.</p>
        </figcaption>
</figure>

      <p>First, a user can insert a URL of a hypermedia-enabled LOD interface. For example, a user can go to <a href="https://hetarchief.be/nl/zoeken/%2A?Filetype%5Bdocument%5D=document">hetarchief.be</a> and select a newspaper as starting point.
After pressing <em>Generate</em>, Comunica fetches the document located on the given URL and follows the embedded pagination controls. While querying, user feedback is provided with request logs, the amount of processed CSV records and bytes.
Next, the user can <em>Copy</em> the CSV output to its clipboard.
Finally, a SPARQL-query can be configured to customize the desired outcome.</p>

      <p data-height="542" data-theme-id="0" data-slug-hash="ebOzXB" data-default-tab="result" data-user="brechtvdv" data-pen-title="Converteren van website naar spreadsheet" class="codepen">See the Pen <a href="https://codepen.io/brechtvdv/pen/ebOzXB/">Download your website as a spreadsheet
</a> by Brecht Van de Vyvere (<a href="https://codepen.io/brechtvdv">@brechtvdv</a>) on <a href="https://codepen.io">CodePen</a>.</p>
      <script async="" src="https://static.codepen.io/assets/embed/ei.js"></script>

    </section>

  <section id="conclusion">
      <h2>Conclusion</h2>

      <p>Data owners can publish their LOD very cost-efficient on their website with JSON-LD snippets. After an initial cost of adding this feature to their website, they can have an always up-to-date dataset with negligible maintenance costs, however, machine clients that query and harvest over websites can introduce unforeseen spikes of activity. Data owners will need to extend their monitoring capabilities to not only focus on human interaction (e.g. Google Analytics) and apply a HTTP caching strategy for stale resources.</p>

      <p>Linked Data services (HDT <span class="references">[<a href="#ref-6">6</a>]</span> file, TPF interface…) with a higher maintenance cost can be created on top of JSON-LD snippets, but these would suffer from scalability problems: OCR texts have bad compression rates, and thus require gigabytes of disk space. With our solution, these OCR texts are published in a separate document keeping the maintenance cost low while harvesting in an automated way is still possible.</p>

      <!--  What are the advantages and disadvantages of the proposed approach? How can the work be compared with others? What are the differences to other cultural heritage LODs like e.g. Europeana?  -->
      <p>The LOD interfaces of the European cultural heritage platform Europeana take an opposite approach from this work: every <a href="https://www.europeana.eu/portal/en/record/2022608/NTM_NTM_UWP_23476.html">subject page</a> contains a title and description annotation for SEO, but the actual machine-readable data is exposed through a seperate record <a href="https://www.europeana.eu/api/v2/record/2022608/NTM_NTM_UWP_23476.json-ld?wskey=XXX">API</a> with API key protection and publicly available <a href="http://sparql.europeana.eu/?default-graph-uri=&amp;query=PREFIX+dc%3A+%3Chttp%3A%2F%2Fpurl.org%2Fdc%2Felements%2F1.1%2F%3E%0D%0APREFIX+edm%3A+%3Chttp%3A%2F%2Fwww.europeana.eu%2Fschemas%2Fedm%2F%3E%0D%0APREFIX+ore%3A+%3Chttp%3A%2F%2Fwww.openarchives.org%2Fore%2Fterms%2F%3E%0D%0ASELECT+*%0D%0AWHERE+%7B%0D%0A%3Fs+%3Fp+%3Chttp%3A%2F%2Fdata.europeana.eu%2Fitem%2F2022608%2FNTM_NTM_UWP_23476%3E+.%0D%0A%7D%0D%0ALIMIT+100&amp;should-sponge=grab-everything&amp;format=text%2Fhtml&amp;timeout=0&amp;debug=on">SPARQL</a> endpoint. Supporting JSON-LD snippets as explained in this work would make the record API obsolete and more importantly, Open Data reusers would have a starting point for querying the SPARQL endpoint.</p>

      <p>In future work, extending Comunica for harvesting Hydra collections would help organizations to improve their collection management. These collections could be defined on their main page of their website improving Open Data discoverability.</p>

      <!--By using our demonstrator, non-technical users are able to extract a data dump from an enriched website.-->

      <!-- The cultural heritage website hetarchief.be showcases an official maintained paged collection of Linked Data Fragments about newspapers. By extending Comunica, in-depth data analysis and federated querying over this dataset is possible. To improve querying speed, Linked Data services ([SPARQL-endpoint](http://semanticweb.org/wiki/SPARQL_endpoint.html), HDT <span class="references">\[<a href="#ref-6">6</a>\]</span> file, TPF interface...) with a higher maintenance cost can be created on top of JSON-LD snippets. Such interfaces would suffer from scalability problems: Optical Character Recognition (OCR) texts have bad compression rates, and thus require gigabytes of disk space. With our solution, these OCR-text are published in a seperate document keeping the maintenance cost low while harvesting in an automated way is still possible. By using our demonstrator, non-technical users are able to extract a data dump from an enriched website. -->

      <!-- To gain traction with an international audience, e.g. the science stories platform ([http://sciencestories.io](http://sciencestories.io)), a reconciliation service could be created with knowledge bases (cfr. Wikidata). 
Next to embedding the data, hypermedia controls or search engine optimization features, also the [International Image Interoperability Framework](https://iiif.io/api/image/2.1/) (IIIF) Image API for sharing images could be described within a JSON-LD snippet for raising the discoverability of this service. IIIF API information already uses JSON-LD to describe its features such as tiling and licensing which makes this an excellent snippet addition helping an organization become more visible on the Web. -->

      <!--In future work, extending Comunica for harvesting Hydra collections would help organizations to improve their collection management. These collections could be defined on their main page of their website improving Open Data discoverability. Also work on supporting multiple views acting as indexes for collections would benefit querying performance on sorting or filtering operations on e.g. geospatial or temporal data.-->
    </section>

</main>

<footer>
<section id="references">
<h2>References</h2>
<dl class="references">
  <dt id="ref-1">[1]</dt>
  <dd resource="https://5stardata.info/en/" typeof="schema:CreativeWork">Berners-Lee, T.: 5 Star Data. <a href="https://5stardata.info/en/">https:/​/​5stardata.info/en/</a> (2009).</dd>
  <dt id="ref-2">[2]</dt>
  <dd resource="https://dx.doi.org/10.1016/j.websem.2016.03.003" typeof="schema:Article">Verborgh, R., Vander Sande, M., Hartig, O., Van Herwegen, J., De Vocht, L., De Meester, B., Haesendonck, G., Colpaert, P.: Triple Pattern Fragments: a Low-cost Knowledge Graph Interface for the Web. Journal of Web Semantics. 37–38, (2016).</dd>
  <dt id="ref-3">[3]</dt>
  <dd resource="https://dx.doi.org/10.1109/MIC.2015.81" typeof="schema:Article">Mika, P.: On Schema.org and Why It Matters for the Web. IEEE Internet Computing. 19, 52–55 (2015).</dd>
  <dt id="ref-4">[4]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-030-00668-6_15" typeof="schema:Article">Taelman, R., Van Herwegen, J., Vander Sande, M., Verborgh, R.: Comunica: a Modular SPARQL Query Engine for the Web. In: Vrandečić, D., Bontcheva, K., Suárez-Figueroa, M.C., Presutti, V., Celino, I., Sabou, M., Kaffee, L.-A., and Simperl, E. (eds.) Proceedings of the 17th International Semantic Web Conference. pp. 239–255. Springer (2018).</dd>
  <dt id="ref-5">[5]</dt>
  <dd resource="#fielding2000architectural" typeof="schema:Book">Fielding, R.T., Taylor, R.N.: Architectural styles and the design of network-based software architectures. University of California, Irvine Irvine, USA (2000).</dd>
  <dt id="ref-6">[6]</dt>
  <dd resource="#Fernndez2013BinaryRR" typeof="schema:Article">Fernández, J.D., Martínez-Prieto, M.A., Gutiérrez, C., Polleres, A., Arias, M.: Binary RDF representation for publication and exchange (HDT). J. Web Sem. 19, 22–41 (2013).</dd>
</dl>
</section>
</footer>

</div>



</body>
</html>
